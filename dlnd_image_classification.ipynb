{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 42 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 7 Name: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMEEPAIX9X2vzXBeiIamsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe390abc0b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    image_min = x.min()\n",
    "    image_max = x.max()\n",
    "    return (x - image_min) / (image_max - image_min)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = None\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global label_encoder\n",
    "    \n",
    "    if label_encoder is None:\n",
    "        label_encoder = preprocessing.LabelBinarizer()\n",
    "        label_encoder.fit(x)\n",
    "    \n",
    "    return label_encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.int32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "#     x_tensor = tf.cast(x_tensor, tf.float32)\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    ## conv\n",
    "    W = tf.Variable(tf.truncated_normal((conv_ksize[0], conv_ksize[1], int(x_tensor.shape[3]), conv_num_outputs), stddev=0.01))\n",
    "    b = tf.Variable(tf.random_normal([conv_num_outputs]))\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    conv = tf.nn.conv2d(x_tensor, W, strides, padding)\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    ## pooling\n",
    "    p_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    p_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    p_padding = 'SAME'\n",
    "    pool = tf.nn.max_pool(conv, p_ksize, p_strides, p_padding)\n",
    "    \n",
    "    return pool\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.shape\n",
    "    d1, d2, d3 = int(shape[1]), int(shape[2]), int(shape[3])\n",
    "    flatten = tf.reshape(x_tensor, [-1, d1*d2*d3])\n",
    "    return flatten\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    W = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.01))\n",
    "    b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    layer = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    W = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.01))\n",
    "    b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor, W), b)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    res = conv2d_maxpool(x, 32, (4, 4), (1, 1), (2, 2), (1, 1))\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    res = conv2d_maxpool(x, 64, (4, 4), (1, 1), (2, 2), (1, 1))\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    res = flatten(res)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    res = fully_conn(res, 64)\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    res = fully_conn(res, 32)\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)    \n",
    "    out = output(res, valid_labels.shape[1])\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch, \n",
    "        y: label_batch, \n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    \n",
    "    v_size = 128\n",
    "    \n",
    "    acc = session.run(accuracy, feed_dict={\n",
    "        x: valid_features[:v_size],\n",
    "        y: valid_labels[:v_size], \n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2033 Accuracy: 0.171875\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1441 Accuracy: 0.164062\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1362 Accuracy: 0.257812\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1103 Accuracy: 0.218750\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0878 Accuracy: 0.218750\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0852 Accuracy: 0.226562\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0722 Accuracy: 0.218750\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0701 Accuracy: 0.179688\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0355 Accuracy: 0.218750\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0546 Accuracy: 0.171875\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.9982 Accuracy: 0.210938\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.0047 Accuracy: 0.203125\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.0091 Accuracy: 0.234375\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9826 Accuracy: 0.257812\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9485 Accuracy: 0.242188\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.9372 Accuracy: 0.257812\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.9093 Accuracy: 0.273438\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.8818 Accuracy: 0.257812\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.8561 Accuracy: 0.250000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8639 Accuracy: 0.226562\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.8357 Accuracy: 0.242188\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.8490 Accuracy: 0.257812\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.8765 Accuracy: 0.234375\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.8335 Accuracy: 0.242188\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.8126 Accuracy: 0.257812\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.7961 Accuracy: 0.203125\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.8423 Accuracy: 0.257812\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.7553 Accuracy: 0.257812\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.7261 Accuracy: 0.281250\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.6872 Accuracy: 0.242188\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.7008 Accuracy: 0.257812\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.7103 Accuracy: 0.257812\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.7255 Accuracy: 0.273438\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.6590 Accuracy: 0.257812\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.6841 Accuracy: 0.265625\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.6530 Accuracy: 0.273438\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.6566 Accuracy: 0.242188\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.6168 Accuracy: 0.273438\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.6538 Accuracy: 0.257812\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.6291 Accuracy: 0.289062\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.5949 Accuracy: 0.265625\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.5902 Accuracy: 0.265625\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.5979 Accuracy: 0.304688\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.5552 Accuracy: 0.289062\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.5892 Accuracy: 0.265625\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.5656 Accuracy: 0.281250\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.5780 Accuracy: 0.273438\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.5568 Accuracy: 0.265625\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.5296 Accuracy: 0.281250\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.5974 Accuracy: 0.312500\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.4920 Accuracy: 0.335938\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.5167 Accuracy: 0.281250\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.5343 Accuracy: 0.343750\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.5113 Accuracy: 0.296875\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.5051 Accuracy: 0.335938\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.5440 Accuracy: 0.304688\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.5058 Accuracy: 0.304688\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.5061 Accuracy: 0.335938\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.5366 Accuracy: 0.328125\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.4688 Accuracy: 0.289062\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.4770 Accuracy: 0.289062\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.4384 Accuracy: 0.312500\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.4292 Accuracy: 0.375000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.4362 Accuracy: 0.367188\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.4584 Accuracy: 0.343750\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.4650 Accuracy: 0.359375\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.4442 Accuracy: 0.351562\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.3927 Accuracy: 0.343750\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.4354 Accuracy: 0.328125\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.3944 Accuracy: 0.375000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.3867 Accuracy: 0.375000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.4367 Accuracy: 0.375000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.3721 Accuracy: 0.390625\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.3456 Accuracy: 0.390625\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.3640 Accuracy: 0.367188\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.3375 Accuracy: 0.398438\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.3785 Accuracy: 0.375000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.3260 Accuracy: 0.460938\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.3673 Accuracy: 0.398438\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.3516 Accuracy: 0.437500\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.3588 Accuracy: 0.437500\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.3418 Accuracy: 0.406250\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.3416 Accuracy: 0.429688\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.3076 Accuracy: 0.468750\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.3066 Accuracy: 0.437500\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.3326 Accuracy: 0.414062\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.2871 Accuracy: 0.453125\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.3473 Accuracy: 0.390625\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.3096 Accuracy: 0.382812\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.3040 Accuracy: 0.414062\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.3177 Accuracy: 0.437500\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.3113 Accuracy: 0.398438\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.3068 Accuracy: 0.429688\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.3044 Accuracy: 0.390625\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.2718 Accuracy: 0.421875\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.2690 Accuracy: 0.429688\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.2667 Accuracy: 0.406250\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.1985 Accuracy: 0.421875\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.2803 Accuracy: 0.421875\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.1993 Accuracy: 0.429688\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.2085 Accuracy: 0.453125\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.2251 Accuracy: 0.437500\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.2078 Accuracy: 0.421875\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.2651 Accuracy: 0.453125\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.2260 Accuracy: 0.414062\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.3019 Accuracy: 0.437500\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.1628 Accuracy: 0.468750\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.1762 Accuracy: 0.406250\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.2258 Accuracy: 0.453125\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.2495 Accuracy: 0.468750\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.2319 Accuracy: 0.382812\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.1965 Accuracy: 0.453125\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.2341 Accuracy: 0.421875\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.1860 Accuracy: 0.421875\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.1110 Accuracy: 0.429688\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.2113 Accuracy: 0.406250\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.1722 Accuracy: 0.429688\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.1507 Accuracy: 0.460938\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.1494 Accuracy: 0.437500\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.1799 Accuracy: 0.445312\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.1314 Accuracy: 0.484375\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.1089 Accuracy: 0.468750\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.0908 Accuracy: 0.476562\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.1208 Accuracy: 0.468750\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.1119 Accuracy: 0.492188\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.1116 Accuracy: 0.453125\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.1353 Accuracy: 0.453125\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.0926 Accuracy: 0.476562\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.1404 Accuracy: 0.476562\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.1255 Accuracy: 0.476562\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.1004 Accuracy: 0.453125\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.1118 Accuracy: 0.476562\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.1140 Accuracy: 0.468750\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.1573 Accuracy: 0.476562\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.1136 Accuracy: 0.468750\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.0761 Accuracy: 0.476562\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.0836 Accuracy: 0.453125\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.1124 Accuracy: 0.468750\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.0931 Accuracy: 0.414062\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.0795 Accuracy: 0.445312\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.1198 Accuracy: 0.500000\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.1049 Accuracy: 0.460938\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.0757 Accuracy: 0.468750\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.0828 Accuracy: 0.437500\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.0306 Accuracy: 0.500000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.1015 Accuracy: 0.453125\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.0620 Accuracy: 0.460938\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.0697 Accuracy: 0.460938\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.0710 Accuracy: 0.468750\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.0853 Accuracy: 0.445312\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.0869 Accuracy: 0.453125\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.0954 Accuracy: 0.476562\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.0766 Accuracy: 0.445312\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.0329 Accuracy: 0.468750\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.0776 Accuracy: 0.484375\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.0743 Accuracy: 0.453125\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.0428 Accuracy: 0.476562\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.0363 Accuracy: 0.460938\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.0566 Accuracy: 0.484375\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.0545 Accuracy: 0.453125\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.0318 Accuracy: 0.500000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.0251 Accuracy: 0.476562\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.0424 Accuracy: 0.507812\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.0452 Accuracy: 0.523438\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.0497 Accuracy: 0.492188\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.0035 Accuracy: 0.484375\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.9880 Accuracy: 0.476562\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.9620 Accuracy: 0.460938\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.0622 Accuracy: 0.453125\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.0355 Accuracy: 0.460938\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.9794 Accuracy: 0.468750\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.9902 Accuracy: 0.500000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     1.0088 Accuracy: 0.484375\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.0027 Accuracy: 0.523438\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.9841 Accuracy: 0.515625\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.0017 Accuracy: 0.484375\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.9606 Accuracy: 0.476562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-0ebd1bbc35ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-69b87072f46c>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     })\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3249 Accuracy: 0.101562\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2401 Accuracy: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2403 Accuracy: 0.062500\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.1563 Accuracy: 0.171875\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.1092 Accuracy: 0.257812\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0765 Accuracy: 0.195312\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.8883 Accuracy: 0.289062\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.7544 Accuracy: 0.304688\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.8383 Accuracy: 0.281250\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.8564 Accuracy: 0.257812\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0044 Accuracy: 0.273438\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.8860 Accuracy: 0.304688\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.6214 Accuracy: 0.265625\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.7320 Accuracy: 0.312500\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7959 Accuracy: 0.265625\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9250 Accuracy: 0.328125\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.7692 Accuracy: 0.335938\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.5305 Accuracy: 0.265625\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.6821 Accuracy: 0.304688\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.6876 Accuracy: 0.304688\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8716 Accuracy: 0.328125\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.7676 Accuracy: 0.289062\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.4949 Accuracy: 0.304688\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.7037 Accuracy: 0.359375\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6326 Accuracy: 0.359375\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8595 Accuracy: 0.445312\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.6669 Accuracy: 0.343750\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.4479 Accuracy: 0.367188\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.5980 Accuracy: 0.414062\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.5913 Accuracy: 0.382812\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.7806 Accuracy: 0.382812\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.6783 Accuracy: 0.335938\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.3962 Accuracy: 0.367188\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.5827 Accuracy: 0.398438\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.5733 Accuracy: 0.351562\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.7355 Accuracy: 0.382812\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.6109 Accuracy: 0.367188\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.3834 Accuracy: 0.335938\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.5895 Accuracy: 0.375000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.5495 Accuracy: 0.328125\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.7088 Accuracy: 0.382812\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.6164 Accuracy: 0.343750\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3852 Accuracy: 0.453125\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.5596 Accuracy: 0.398438\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.4840 Accuracy: 0.335938\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.6762 Accuracy: 0.406250\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.6134 Accuracy: 0.367188\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.3242 Accuracy: 0.437500\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.4894 Accuracy: 0.382812\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.4775 Accuracy: 0.382812\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.6355 Accuracy: 0.398438\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.5442 Accuracy: 0.398438\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.3125 Accuracy: 0.414062\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.4637 Accuracy: 0.398438\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.4262 Accuracy: 0.414062\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5520 Accuracy: 0.453125\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.5635 Accuracy: 0.390625\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.2816 Accuracy: 0.429688\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.4485 Accuracy: 0.476562\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4164 Accuracy: 0.414062\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5482 Accuracy: 0.406250\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.5215 Accuracy: 0.382812\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.2579 Accuracy: 0.445312\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.4024 Accuracy: 0.453125\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.4627 Accuracy: 0.367188\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5489 Accuracy: 0.414062\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.5189 Accuracy: 0.437500\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2112 Accuracy: 0.398438\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.4025 Accuracy: 0.468750\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.3348 Accuracy: 0.445312\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5106 Accuracy: 0.492188\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.5072 Accuracy: 0.437500\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.1885 Accuracy: 0.429688\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.4274 Accuracy: 0.476562\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.3156 Accuracy: 0.445312\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4645 Accuracy: 0.476562\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.4124 Accuracy: 0.460938\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1892 Accuracy: 0.453125\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.3512 Accuracy: 0.476562\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.3255 Accuracy: 0.429688\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4228 Accuracy: 0.468750\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.3794 Accuracy: 0.453125\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1561 Accuracy: 0.445312\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.3556 Accuracy: 0.484375\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.2914 Accuracy: 0.453125\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.4303 Accuracy: 0.492188\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.3472 Accuracy: 0.500000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.1459 Accuracy: 0.476562\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.3505 Accuracy: 0.507812\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.2393 Accuracy: 0.468750\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3980 Accuracy: 0.500000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.3561 Accuracy: 0.507812\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.1415 Accuracy: 0.484375\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.2955 Accuracy: 0.492188\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.2542 Accuracy: 0.507812\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3897 Accuracy: 0.492188\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.3261 Accuracy: 0.468750\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.0744 Accuracy: 0.421875\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.3073 Accuracy: 0.476562\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.2297 Accuracy: 0.468750\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3459 Accuracy: 0.515625\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.3121 Accuracy: 0.484375\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.1675 Accuracy: 0.468750\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.2667 Accuracy: 0.476562\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.2388 Accuracy: 0.460938\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.2936 Accuracy: 0.500000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.3188 Accuracy: 0.476562\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.0823 Accuracy: 0.492188\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.2663 Accuracy: 0.429688\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.2417 Accuracy: 0.445312\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.3882 Accuracy: 0.445312\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.2728 Accuracy: 0.460938\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.1318 Accuracy: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.3095 Accuracy: 0.445312\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.1842 Accuracy: 0.500000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.3260 Accuracy: 0.468750\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.2481 Accuracy: 0.500000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.0565 Accuracy: 0.468750\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.2441 Accuracy: 0.523438\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.1876 Accuracy: 0.484375\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.3208 Accuracy: 0.500000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.2199 Accuracy: 0.507812\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.0953 Accuracy: 0.453125\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.2602 Accuracy: 0.484375\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.1863 Accuracy: 0.515625\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.2317 Accuracy: 0.546875\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.2045 Accuracy: 0.492188\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.0793 Accuracy: 0.445312\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.2108 Accuracy: 0.492188\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.1566 Accuracy: 0.484375\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.2588 Accuracy: 0.507812\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.1918 Accuracy: 0.453125\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.0689 Accuracy: 0.500000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.2168 Accuracy: 0.500000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.1605 Accuracy: 0.500000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.2392 Accuracy: 0.492188\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.2005 Accuracy: 0.453125\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.0810 Accuracy: 0.531250\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.2003 Accuracy: 0.468750\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.1292 Accuracy: 0.445312\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.2453 Accuracy: 0.507812\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.1657 Accuracy: 0.523438\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.0288 Accuracy: 0.523438\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.2475 Accuracy: 0.523438\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.0858 Accuracy: 0.460938\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.2602 Accuracy: 0.570312\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.1631 Accuracy: 0.492188\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.9678 Accuracy: 0.492188\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.1734 Accuracy: 0.500000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.0935 Accuracy: 0.484375\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.2461 Accuracy: 0.523438\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.1439 Accuracy: 0.507812\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.0179 Accuracy: 0.523438\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.1800 Accuracy: 0.523438\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.0813 Accuracy: 0.468750\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.2074 Accuracy: 0.562500\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.2156 Accuracy: 0.507812\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.9752 Accuracy: 0.484375\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.1435 Accuracy: 0.515625\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.0651 Accuracy: 0.507812\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.1790 Accuracy: 0.554688\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.1242 Accuracy: 0.507812\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.9275 Accuracy: 0.507812\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.1134 Accuracy: 0.539062\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.1010 Accuracy: 0.531250\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.1662 Accuracy: 0.515625\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.1077 Accuracy: 0.492188\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.9709 Accuracy: 0.531250\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.1353 Accuracy: 0.507812\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.0676 Accuracy: 0.523438\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.1562 Accuracy: 0.492188\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.1394 Accuracy: 0.515625\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.9471 Accuracy: 0.492188\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.1106 Accuracy: 0.554688\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.0546 Accuracy: 0.515625\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.1341 Accuracy: 0.539062\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.0798 Accuracy: 0.531250\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.0098 Accuracy: 0.554688\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.0866 Accuracy: 0.523438\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.9903 Accuracy: 0.484375\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.1565 Accuracy: 0.578125\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.1123 Accuracy: 0.500000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.9645 Accuracy: 0.539062\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.1147 Accuracy: 0.507812\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.0202 Accuracy: 0.468750\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.1420 Accuracy: 0.539062\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.0527 Accuracy: 0.539062\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.8971 Accuracy: 0.515625\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.1132 Accuracy: 0.570312\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.0305 Accuracy: 0.546875\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.1491 Accuracy: 0.523438\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.0577 Accuracy: 0.531250\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.9168 Accuracy: 0.531250\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.0902 Accuracy: 0.539062\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.0204 Accuracy: 0.539062\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.1360 Accuracy: 0.562500\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     1.0075 Accuracy: 0.507812\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.9220 Accuracy: 0.523438\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.0895 Accuracy: 0.546875\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.0168 Accuracy: 0.500000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.1139 Accuracy: 0.507812\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.9983 Accuracy: 0.539062\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.9325 Accuracy: 0.539062\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.0502 Accuracy: 0.523438\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.9715 Accuracy: 0.523438\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.0931 Accuracy: 0.523438\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     1.0019 Accuracy: 0.554688\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.9071 Accuracy: 0.523438\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.0239 Accuracy: 0.539062\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.9936 Accuracy: 0.468750\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.0612 Accuracy: 0.515625\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     1.0178 Accuracy: 0.531250\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.8837 Accuracy: 0.492188\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.0426 Accuracy: 0.523438\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.9745 Accuracy: 0.500000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.0984 Accuracy: 0.562500\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.9998 Accuracy: 0.539062\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.8440 Accuracy: 0.523438\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.0387 Accuracy: 0.531250\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.9538 Accuracy: 0.531250\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.0591 Accuracy: 0.500000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     1.0855 Accuracy: 0.523438\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.8814 Accuracy: 0.507812\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.9707 Accuracy: 0.507812\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.9458 Accuracy: 0.507812\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.0451 Accuracy: 0.562500\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     1.0032 Accuracy: 0.468750\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.8426 Accuracy: 0.562500\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.0031 Accuracy: 0.515625\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.9494 Accuracy: 0.460938\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.9885 Accuracy: 0.515625\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.9677 Accuracy: 0.500000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.8819 Accuracy: 0.492188\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.9993 Accuracy: 0.546875\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.9540 Accuracy: 0.468750\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.9661 Accuracy: 0.593750\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.9702 Accuracy: 0.570312\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.7930 Accuracy: 0.546875\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.9857 Accuracy: 0.531250\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.9433 Accuracy: 0.531250\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.9698 Accuracy: 0.593750\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.9861 Accuracy: 0.500000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.8064 Accuracy: 0.539062\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.0053 Accuracy: 0.570312\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.9020 Accuracy: 0.531250\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.0286 Accuracy: 0.578125\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.9802 Accuracy: 0.515625\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.8079 Accuracy: 0.531250\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.0244 Accuracy: 0.578125\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.9379 Accuracy: 0.468750\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.0258 Accuracy: 0.554688\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.9305 Accuracy: 0.492188\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.8518 Accuracy: 0.539062\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.9793 Accuracy: 0.562500\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.9619 Accuracy: 0.531250\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.9792 Accuracy: 0.578125\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.8691 Accuracy: 0.492188\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.8449 Accuracy: 0.562500\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.0036 Accuracy: 0.515625\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.9094 Accuracy: 0.484375\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.9595 Accuracy: 0.570312\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.9349 Accuracy: 0.523438\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.8071 Accuracy: 0.515625\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.9890 Accuracy: 0.531250\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.9024 Accuracy: 0.476562\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.9790 Accuracy: 0.546875\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.9278 Accuracy: 0.523438\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.8736 Accuracy: 0.515625\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.9405 Accuracy: 0.531250\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.9076 Accuracy: 0.531250\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.9804 Accuracy: 0.570312\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.9681 Accuracy: 0.570312\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.8234 Accuracy: 0.515625\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.9418 Accuracy: 0.570312\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.8980 Accuracy: 0.507812\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.9472 Accuracy: 0.578125\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.8714 Accuracy: 0.531250\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.8408 Accuracy: 0.562500\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.9350 Accuracy: 0.562500\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.8872 Accuracy: 0.515625\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.9529 Accuracy: 0.554688\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.8922 Accuracy: 0.539062\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.8566 Accuracy: 0.593750\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.9295 Accuracy: 0.570312\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.8618 Accuracy: 0.570312\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9939 Accuracy: 0.585938\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.8466 Accuracy: 0.531250\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.8294 Accuracy: 0.546875\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.9145 Accuracy: 0.578125\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.8757 Accuracy: 0.531250\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.9499 Accuracy: 0.562500\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.8660 Accuracy: 0.539062\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.8058 Accuracy: 0.601562\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.9293 Accuracy: 0.546875\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.8540 Accuracy: 0.554688\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9256 Accuracy: 0.562500\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.8515 Accuracy: 0.570312\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.8084 Accuracy: 0.562500\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.9157 Accuracy: 0.531250\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.8648 Accuracy: 0.515625\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.9285 Accuracy: 0.539062\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.8047 Accuracy: 0.523438\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.7545 Accuracy: 0.570312\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.9792 Accuracy: 0.585938\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.8698 Accuracy: 0.500000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9258 Accuracy: 0.546875\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.8190 Accuracy: 0.562500\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.7621 Accuracy: 0.539062\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.9124 Accuracy: 0.539062\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.8728 Accuracy: 0.515625\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.9153 Accuracy: 0.578125\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.8227 Accuracy: 0.578125\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.7329 Accuracy: 0.546875\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.9441 Accuracy: 0.632812\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.8625 Accuracy: 0.531250\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.9001 Accuracy: 0.570312\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.7994 Accuracy: 0.562500\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.7417 Accuracy: 0.593750\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.9042 Accuracy: 0.539062\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.8706 Accuracy: 0.546875\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.9148 Accuracy: 0.578125\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.8309 Accuracy: 0.523438\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.7456 Accuracy: 0.585938\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.8983 Accuracy: 0.578125\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.8551 Accuracy: 0.523438\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.9282 Accuracy: 0.625000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.7934 Accuracy: 0.578125\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.7012 Accuracy: 0.570312\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.9026 Accuracy: 0.578125\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.8392 Accuracy: 0.539062\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.9315 Accuracy: 0.625000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.7939 Accuracy: 0.562500\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.6997 Accuracy: 0.523438\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.8704 Accuracy: 0.546875\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.8254 Accuracy: 0.523438\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.9334 Accuracy: 0.640625\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.8325 Accuracy: 0.585938\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.7034 Accuracy: 0.585938\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.8785 Accuracy: 0.578125\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.8349 Accuracy: 0.554688\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.8831 Accuracy: 0.562500\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.7751 Accuracy: 0.578125\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.6983 Accuracy: 0.570312\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.8492 Accuracy: 0.562500\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.8422 Accuracy: 0.570312\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.8952 Accuracy: 0.609375\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.7702 Accuracy: 0.554688\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.6809 Accuracy: 0.546875\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.9174 Accuracy: 0.570312\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.8097 Accuracy: 0.585938\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.8671 Accuracy: 0.625000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.7714 Accuracy: 0.546875\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.7426 Accuracy: 0.507812\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.8429 Accuracy: 0.546875\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.8113 Accuracy: 0.570312\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.8759 Accuracy: 0.593750\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.8135 Accuracy: 0.515625\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.7108 Accuracy: 0.562500\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.8636 Accuracy: 0.570312\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.8101 Accuracy: 0.562500\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.8743 Accuracy: 0.609375\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.7470 Accuracy: 0.531250\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.7083 Accuracy: 0.507812\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.8500 Accuracy: 0.609375\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.8169 Accuracy: 0.562500\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.9030 Accuracy: 0.593750\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.7556 Accuracy: 0.531250\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.7297 Accuracy: 0.562500\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.8689 Accuracy: 0.593750\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.7864 Accuracy: 0.546875\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.8631 Accuracy: 0.609375\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.7755 Accuracy: 0.570312\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.7002 Accuracy: 0.546875\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.8178 Accuracy: 0.570312\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.8062 Accuracy: 0.593750\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.9057 Accuracy: 0.625000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.8107 Accuracy: 0.601562\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.6997 Accuracy: 0.593750\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.8396 Accuracy: 0.632812\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.7763 Accuracy: 0.546875\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.8964 Accuracy: 0.664062\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.7495 Accuracy: 0.554688\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.6825 Accuracy: 0.570312\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.8328 Accuracy: 0.570312\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.7867 Accuracy: 0.562500\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.8901 Accuracy: 0.617188\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.7384 Accuracy: 0.570312\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.6443 Accuracy: 0.593750\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.8367 Accuracy: 0.640625\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.7884 Accuracy: 0.593750\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.8862 Accuracy: 0.609375\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.7214 Accuracy: 0.554688\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.6482 Accuracy: 0.562500\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.8223 Accuracy: 0.640625\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.8344 Accuracy: 0.523438\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.8526 Accuracy: 0.609375\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.7548 Accuracy: 0.578125\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.6860 Accuracy: 0.570312\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.8044 Accuracy: 0.640625\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.8356 Accuracy: 0.570312\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.8605 Accuracy: 0.601562\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.7177 Accuracy: 0.585938\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.6988 Accuracy: 0.570312\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.8596 Accuracy: 0.609375\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.7963 Accuracy: 0.593750\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.8384 Accuracy: 0.601562\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.7424 Accuracy: 0.585938\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.6685 Accuracy: 0.523438\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.8334 Accuracy: 0.632812\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.7737 Accuracy: 0.578125\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.8778 Accuracy: 0.617188\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.7271 Accuracy: 0.554688\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.6560 Accuracy: 0.593750\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.8979 Accuracy: 0.617188\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.7427 Accuracy: 0.609375\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.8289 Accuracy: 0.625000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.7102 Accuracy: 0.554688\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.6616 Accuracy: 0.562500\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.7890 Accuracy: 0.617188\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.8065 Accuracy: 0.585938\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.8814 Accuracy: 0.625000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.6902 Accuracy: 0.531250\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.6518 Accuracy: 0.578125\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.7741 Accuracy: 0.617188\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.7610 Accuracy: 0.562500\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.8461 Accuracy: 0.640625\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.7187 Accuracy: 0.531250\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.6455 Accuracy: 0.593750\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.8101 Accuracy: 0.632812\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.7712 Accuracy: 0.554688\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.8596 Accuracy: 0.609375\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.7205 Accuracy: 0.578125\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.6490 Accuracy: 0.609375\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.7794 Accuracy: 0.617188\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.7440 Accuracy: 0.585938\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.8149 Accuracy: 0.609375\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.7263 Accuracy: 0.578125\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.6335 Accuracy: 0.546875\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.7902 Accuracy: 0.585938\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.7573 Accuracy: 0.539062\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.8237 Accuracy: 0.593750\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.7220 Accuracy: 0.578125\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.6289 Accuracy: 0.593750\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.8041 Accuracy: 0.585938\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.7609 Accuracy: 0.578125\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.8174 Accuracy: 0.632812\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.6724 Accuracy: 0.554688\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.5944 Accuracy: 0.546875\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.7892 Accuracy: 0.578125\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.7635 Accuracy: 0.585938\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.7844 Accuracy: 0.593750\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.6687 Accuracy: 0.578125\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.6182 Accuracy: 0.609375\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.8052 Accuracy: 0.593750\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.7323 Accuracy: 0.585938\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.8275 Accuracy: 0.593750\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.6923 Accuracy: 0.562500\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.6287 Accuracy: 0.585938\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.7937 Accuracy: 0.554688\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.7477 Accuracy: 0.554688\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.7948 Accuracy: 0.617188\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.6701 Accuracy: 0.562500\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.6452 Accuracy: 0.601562\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.8066 Accuracy: 0.632812\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.7298 Accuracy: 0.554688\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.7982 Accuracy: 0.585938\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.6723 Accuracy: 0.523438\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.6205 Accuracy: 0.554688\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.7424 Accuracy: 0.601562\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.7231 Accuracy: 0.585938\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.7775 Accuracy: 0.632812\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.6696 Accuracy: 0.523438\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.6232 Accuracy: 0.562500\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.7924 Accuracy: 0.578125\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.6937 Accuracy: 0.570312\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.7876 Accuracy: 0.640625\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.6915 Accuracy: 0.546875\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.6492 Accuracy: 0.601562\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.7904 Accuracy: 0.585938\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.7117 Accuracy: 0.554688\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.7745 Accuracy: 0.593750\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.6614 Accuracy: 0.554688\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.6134 Accuracy: 0.648438\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.7660 Accuracy: 0.539062\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.7171 Accuracy: 0.546875\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.7489 Accuracy: 0.562500\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.7049 Accuracy: 0.593750\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.5807 Accuracy: 0.570312\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.7605 Accuracy: 0.578125\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.7630 Accuracy: 0.601562\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.7771 Accuracy: 0.593750\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.6705 Accuracy: 0.562500\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.5916 Accuracy: 0.617188\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.7975 Accuracy: 0.609375\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.7218 Accuracy: 0.570312\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.7542 Accuracy: 0.578125\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.6840 Accuracy: 0.531250\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.6010 Accuracy: 0.593750\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.7421 Accuracy: 0.570312\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.7532 Accuracy: 0.531250\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.8213 Accuracy: 0.609375\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.6785 Accuracy: 0.578125\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.6293 Accuracy: 0.562500\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.7656 Accuracy: 0.601562\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.7553 Accuracy: 0.625000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.7637 Accuracy: 0.617188\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.6589 Accuracy: 0.578125\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.6159 Accuracy: 0.617188\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.7587 Accuracy: 0.617188\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.6943 Accuracy: 0.593750\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.7704 Accuracy: 0.609375\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.6438 Accuracy: 0.609375\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.5818 Accuracy: 0.601562\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.7836 Accuracy: 0.625000\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.7082 Accuracy: 0.601562\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.7525 Accuracy: 0.593750\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.6395 Accuracy: 0.562500\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.6110 Accuracy: 0.601562\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.7793 Accuracy: 0.625000\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.7091 Accuracy: 0.554688\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.7679 Accuracy: 0.656250\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.6111 Accuracy: 0.554688\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.6375 Accuracy: 0.601562\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.7699 Accuracy: 0.609375\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.7024 Accuracy: 0.570312\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.7534 Accuracy: 0.625000\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.6319 Accuracy: 0.585938\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.6310 Accuracy: 0.632812\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.7861 Accuracy: 0.625000\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.7169 Accuracy: 0.593750\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.7707 Accuracy: 0.617188\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.6251 Accuracy: 0.593750\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.6076 Accuracy: 0.617188\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.7230 Accuracy: 0.593750\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.7220 Accuracy: 0.593750\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.7114 Accuracy: 0.640625\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.6219 Accuracy: 0.515625\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.5792 Accuracy: 0.609375\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.7529 Accuracy: 0.664062\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.7205 Accuracy: 0.570312\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.7453 Accuracy: 0.640625\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.6638 Accuracy: 0.593750\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.5785 Accuracy: 0.640625\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.7335 Accuracy: 0.632812\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.6624 Accuracy: 0.609375\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.7196 Accuracy: 0.648438\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.5858 Accuracy: 0.562500\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.5703 Accuracy: 0.625000\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.7365 Accuracy: 0.648438\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.6515 Accuracy: 0.578125\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.7689 Accuracy: 0.601562\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.6213 Accuracy: 0.609375\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.6216 Accuracy: 0.570312\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.7253 Accuracy: 0.625000\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.6773 Accuracy: 0.625000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.7281 Accuracy: 0.632812\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.6494 Accuracy: 0.640625\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.5713 Accuracy: 0.593750\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.7519 Accuracy: 0.648438\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.6371 Accuracy: 0.601562\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.7078 Accuracy: 0.640625\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.6605 Accuracy: 0.593750\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.5689 Accuracy: 0.632812\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.7026 Accuracy: 0.609375\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.6240 Accuracy: 0.593750\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.7502 Accuracy: 0.609375\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.6838 Accuracy: 0.601562\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.5513 Accuracy: 0.578125\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.7106 Accuracy: 0.609375\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.6345 Accuracy: 0.609375\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.7200 Accuracy: 0.570312\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.6077 Accuracy: 0.570312\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.5707 Accuracy: 0.585938\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.7083 Accuracy: 0.585938\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.6510 Accuracy: 0.617188\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.7067 Accuracy: 0.593750\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.6011 Accuracy: 0.593750\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.5845 Accuracy: 0.570312\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.7307 Accuracy: 0.609375\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.6451 Accuracy: 0.578125\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.6631 Accuracy: 0.593750\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.6299 Accuracy: 0.554688\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.5876 Accuracy: 0.609375\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.7310 Accuracy: 0.632812\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.6523 Accuracy: 0.601562\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.6810 Accuracy: 0.640625\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.5875 Accuracy: 0.531250\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.5836 Accuracy: 0.593750\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.7518 Accuracy: 0.648438\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.6598 Accuracy: 0.617188\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.7111 Accuracy: 0.640625\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.5915 Accuracy: 0.562500\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.5744 Accuracy: 0.570312\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.6945 Accuracy: 0.585938\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.6672 Accuracy: 0.625000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.7131 Accuracy: 0.609375\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.6306 Accuracy: 0.617188\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.5632 Accuracy: 0.617188\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.7201 Accuracy: 0.625000\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.5964 Accuracy: 0.546875\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.7139 Accuracy: 0.593750\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.6062 Accuracy: 0.570312\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.5619 Accuracy: 0.601562\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.7092 Accuracy: 0.593750\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.6335 Accuracy: 0.601562\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.6997 Accuracy: 0.593750\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.6121 Accuracy: 0.632812\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.5084 Accuracy: 0.609375\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.7048 Accuracy: 0.625000\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.6146 Accuracy: 0.554688\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.6648 Accuracy: 0.601562\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.6009 Accuracy: 0.562500\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.5925 Accuracy: 0.593750\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.7103 Accuracy: 0.640625\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.5991 Accuracy: 0.562500\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.6601 Accuracy: 0.585938\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.5801 Accuracy: 0.585938\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.5787 Accuracy: 0.593750\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.7328 Accuracy: 0.640625\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.5939 Accuracy: 0.578125\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.6897 Accuracy: 0.609375\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.6014 Accuracy: 0.625000\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.5377 Accuracy: 0.648438\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.6971 Accuracy: 0.656250\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.6224 Accuracy: 0.640625\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.6585 Accuracy: 0.656250\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.5936 Accuracy: 0.585938\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.5777 Accuracy: 0.617188\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.6756 Accuracy: 0.632812\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.6232 Accuracy: 0.578125\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.6738 Accuracy: 0.648438\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.5845 Accuracy: 0.554688\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.5502 Accuracy: 0.593750\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.6835 Accuracy: 0.648438\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.6097 Accuracy: 0.585938\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.6803 Accuracy: 0.585938\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.5656 Accuracy: 0.585938\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.5531 Accuracy: 0.585938\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.6613 Accuracy: 0.625000\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.5823 Accuracy: 0.640625\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.6384 Accuracy: 0.593750\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.6045 Accuracy: 0.546875\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.5860 Accuracy: 0.578125\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.6614 Accuracy: 0.625000\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.5903 Accuracy: 0.601562\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.6690 Accuracy: 0.632812\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.5627 Accuracy: 0.570312\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.5722 Accuracy: 0.562500\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.6788 Accuracy: 0.648438\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.6151 Accuracy: 0.609375\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.6844 Accuracy: 0.593750\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.5720 Accuracy: 0.554688\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.5547 Accuracy: 0.625000\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.6754 Accuracy: 0.625000\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.6105 Accuracy: 0.601562\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.6626 Accuracy: 0.671875\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.5606 Accuracy: 0.585938\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.5316 Accuracy: 0.640625\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.6291 Accuracy: 0.593750\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.6097 Accuracy: 0.554688\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.6593 Accuracy: 0.593750\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.5733 Accuracy: 0.539062\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.4883 Accuracy: 0.593750\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.6368 Accuracy: 0.617188\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.6253 Accuracy: 0.593750\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.6894 Accuracy: 0.601562\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.5868 Accuracy: 0.585938\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.5518 Accuracy: 0.609375\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.6535 Accuracy: 0.617188\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.6187 Accuracy: 0.648438\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.6486 Accuracy: 0.632812\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.5459 Accuracy: 0.609375\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.5024 Accuracy: 0.625000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.6448 Accuracy: 0.632812\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.5786 Accuracy: 0.585938\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.6622 Accuracy: 0.593750\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.5316 Accuracy: 0.578125\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.5015 Accuracy: 0.609375\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.6853 Accuracy: 0.648438\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.6036 Accuracy: 0.617188\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.6221 Accuracy: 0.656250\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.5711 Accuracy: 0.585938\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.4957 Accuracy: 0.625000\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.6522 Accuracy: 0.640625\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.5737 Accuracy: 0.632812\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.6539 Accuracy: 0.632812\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.5729 Accuracy: 0.601562\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.5338 Accuracy: 0.617188\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.6481 Accuracy: 0.640625\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.6077 Accuracy: 0.601562\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.6392 Accuracy: 0.625000\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.5396 Accuracy: 0.609375\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.4878 Accuracy: 0.593750\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.6487 Accuracy: 0.656250\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.5756 Accuracy: 0.640625\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.6497 Accuracy: 0.640625\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.5694 Accuracy: 0.593750\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.5374 Accuracy: 0.617188\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.6393 Accuracy: 0.664062\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.5591 Accuracy: 0.593750\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.6393 Accuracy: 0.625000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.5748 Accuracy: 0.585938\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.4759 Accuracy: 0.562500\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.6559 Accuracy: 0.601562\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.5537 Accuracy: 0.617188\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.6167 Accuracy: 0.640625\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.5424 Accuracy: 0.570312\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.4786 Accuracy: 0.585938\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.6497 Accuracy: 0.625000\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.6135 Accuracy: 0.625000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.6327 Accuracy: 0.648438\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.5468 Accuracy: 0.570312\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.5035 Accuracy: 0.585938\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.6679 Accuracy: 0.601562\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.5744 Accuracy: 0.570312\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.6673 Accuracy: 0.625000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.5375 Accuracy: 0.593750\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.5284 Accuracy: 0.585938\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.6303 Accuracy: 0.593750\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.5892 Accuracy: 0.632812\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.6160 Accuracy: 0.562500\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.5869 Accuracy: 0.578125\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.4858 Accuracy: 0.609375\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.6226 Accuracy: 0.648438\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.5556 Accuracy: 0.625000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.6511 Accuracy: 0.617188\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.5516 Accuracy: 0.570312\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.5038 Accuracy: 0.625000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.6260 Accuracy: 0.570312\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.5736 Accuracy: 0.570312\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.6163 Accuracy: 0.601562\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.5406 Accuracy: 0.578125\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.5071 Accuracy: 0.593750\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.5900 Accuracy: 0.570312\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.5456 Accuracy: 0.625000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.5960 Accuracy: 0.632812\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.5475 Accuracy: 0.554688\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.4778 Accuracy: 0.601562\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.6447 Accuracy: 0.617188\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.5680 Accuracy: 0.562500\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.6230 Accuracy: 0.578125\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.5451 Accuracy: 0.601562\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.5176 Accuracy: 0.578125\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.6375 Accuracy: 0.609375\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.5396 Accuracy: 0.585938\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.6205 Accuracy: 0.617188\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.5528 Accuracy: 0.570312\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.5275 Accuracy: 0.617188\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.6168 Accuracy: 0.609375\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.5570 Accuracy: 0.625000\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.6241 Accuracy: 0.625000\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.5699 Accuracy: 0.617188\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.4724 Accuracy: 0.570312\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.6229 Accuracy: 0.625000\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.5405 Accuracy: 0.570312\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.6232 Accuracy: 0.632812\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.5827 Accuracy: 0.578125\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.5218 Accuracy: 0.593750\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.6287 Accuracy: 0.625000\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.5601 Accuracy: 0.601562\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.6204 Accuracy: 0.593750\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.5544 Accuracy: 0.562500\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.5066 Accuracy: 0.578125\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.5974 Accuracy: 0.625000\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.5272 Accuracy: 0.609375\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.6120 Accuracy: 0.601562\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.5253 Accuracy: 0.578125\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.5069 Accuracy: 0.601562\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.5773 Accuracy: 0.609375\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.5685 Accuracy: 0.593750\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.6160 Accuracy: 0.601562\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.5278 Accuracy: 0.570312\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.5138 Accuracy: 0.601562\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.5895 Accuracy: 0.632812\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.5356 Accuracy: 0.570312\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.6663 Accuracy: 0.640625\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.4944 Accuracy: 0.585938\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.4810 Accuracy: 0.601562\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.5677 Accuracy: 0.625000\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.5345 Accuracy: 0.625000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.6064 Accuracy: 0.640625\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.5036 Accuracy: 0.578125\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.5003 Accuracy: 0.617188\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.5683 Accuracy: 0.617188\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.5204 Accuracy: 0.617188\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.6509 Accuracy: 0.632812\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.4985 Accuracy: 0.601562\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.4962 Accuracy: 0.632812\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.5461 Accuracy: 0.585938\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.5345 Accuracy: 0.601562\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.6453 Accuracy: 0.625000\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.5193 Accuracy: 0.601562\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.4995 Accuracy: 0.609375\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.5534 Accuracy: 0.648438\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.5506 Accuracy: 0.625000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.6202 Accuracy: 0.632812\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.5163 Accuracy: 0.562500\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.4721 Accuracy: 0.640625\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.5596 Accuracy: 0.640625\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.5244 Accuracy: 0.609375\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.6323 Accuracy: 0.625000\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.5206 Accuracy: 0.570312\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.4972 Accuracy: 0.617188\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.5533 Accuracy: 0.617188\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.4955 Accuracy: 0.593750\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.6238 Accuracy: 0.625000\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.5149 Accuracy: 0.585938\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.4658 Accuracy: 0.625000\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.5457 Accuracy: 0.640625\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.5268 Accuracy: 0.585938\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.5952 Accuracy: 0.648438\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.5014 Accuracy: 0.593750\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.4885 Accuracy: 0.648438\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.5715 Accuracy: 0.640625\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.4977 Accuracy: 0.625000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.6068 Accuracy: 0.625000\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.5320 Accuracy: 0.601562\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.4671 Accuracy: 0.648438\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.5468 Accuracy: 0.617188\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.5147 Accuracy: 0.664062\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.5657 Accuracy: 0.632812\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.5283 Accuracy: 0.601562\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.4526 Accuracy: 0.625000\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.5468 Accuracy: 0.648438\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.4951 Accuracy: 0.632812\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.6053 Accuracy: 0.632812\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.4778 Accuracy: 0.601562\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.4547 Accuracy: 0.632812\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.5481 Accuracy: 0.640625\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.5035 Accuracy: 0.617188\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.6218 Accuracy: 0.617188\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.5014 Accuracy: 0.578125\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.4499 Accuracy: 0.625000\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.5359 Accuracy: 0.625000\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.5091 Accuracy: 0.617188\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.6134 Accuracy: 0.625000\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.5197 Accuracy: 0.601562\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.4694 Accuracy: 0.601562\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.5707 Accuracy: 0.632812\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.4780 Accuracy: 0.625000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.5970 Accuracy: 0.640625\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.4795 Accuracy: 0.625000\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.4388 Accuracy: 0.648438\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.5267 Accuracy: 0.648438\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.4695 Accuracy: 0.648438\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.6027 Accuracy: 0.640625\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.4946 Accuracy: 0.625000\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.4676 Accuracy: 0.656250\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.5403 Accuracy: 0.632812\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.5195 Accuracy: 0.609375\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.6171 Accuracy: 0.640625\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.4969 Accuracy: 0.609375\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.4539 Accuracy: 0.601562\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.5011 Accuracy: 0.617188\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.4846 Accuracy: 0.617188\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.6064 Accuracy: 0.640625\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.5042 Accuracy: 0.609375\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.4293 Accuracy: 0.593750\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.4884 Accuracy: 0.632812\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.4712 Accuracy: 0.640625\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.6276 Accuracy: 0.617188\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.4928 Accuracy: 0.601562\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.4408 Accuracy: 0.625000\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.4960 Accuracy: 0.656250\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.4721 Accuracy: 0.609375\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.6139 Accuracy: 0.625000\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.4769 Accuracy: 0.609375\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.4413 Accuracy: 0.648438\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.4900 Accuracy: 0.640625\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.4825 Accuracy: 0.625000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.6368 Accuracy: 0.625000\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.4712 Accuracy: 0.632812\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.4285 Accuracy: 0.656250\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.5253 Accuracy: 0.648438\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.4349 Accuracy: 0.640625\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.5959 Accuracy: 0.625000\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.4836 Accuracy: 0.609375\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.4564 Accuracy: 0.664062\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.5220 Accuracy: 0.648438\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.4781 Accuracy: 0.640625\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.6166 Accuracy: 0.609375\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.4660 Accuracy: 0.632812\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.4175 Accuracy: 0.609375\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.4852 Accuracy: 0.679688\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.4740 Accuracy: 0.632812\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.5699 Accuracy: 0.648438\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.4787 Accuracy: 0.640625\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.4490 Accuracy: 0.648438\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.5177 Accuracy: 0.601562\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.4955 Accuracy: 0.593750\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.5938 Accuracy: 0.609375\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.4662 Accuracy: 0.609375\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.4456 Accuracy: 0.648438\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.5307 Accuracy: 0.617188\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.4622 Accuracy: 0.632812\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.5658 Accuracy: 0.617188\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.4806 Accuracy: 0.609375\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.4216 Accuracy: 0.656250\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.4818 Accuracy: 0.640625\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.4665 Accuracy: 0.648438\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.5899 Accuracy: 0.617188\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.4718 Accuracy: 0.648438\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.4562 Accuracy: 0.632812\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.5132 Accuracy: 0.554688\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.4689 Accuracy: 0.648438\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.5875 Accuracy: 0.632812\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.4357 Accuracy: 0.578125\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.4381 Accuracy: 0.640625\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.5140 Accuracy: 0.656250\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.4864 Accuracy: 0.640625\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.5990 Accuracy: 0.617188\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.4850 Accuracy: 0.664062\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.4210 Accuracy: 0.664062\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.5324 Accuracy: 0.656250\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.4660 Accuracy: 0.648438\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.5687 Accuracy: 0.617188\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.4540 Accuracy: 0.617188\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.4113 Accuracy: 0.671875\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.5202 Accuracy: 0.648438\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.5129 Accuracy: 0.570312\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.5552 Accuracy: 0.671875\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.4846 Accuracy: 0.625000\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.4329 Accuracy: 0.640625\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.5074 Accuracy: 0.664062\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.4908 Accuracy: 0.609375\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.5611 Accuracy: 0.593750\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.4385 Accuracy: 0.601562\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.4166 Accuracy: 0.640625\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.5034 Accuracy: 0.617188\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.4594 Accuracy: 0.601562\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.5754 Accuracy: 0.632812\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.4551 Accuracy: 0.609375\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.4182 Accuracy: 0.625000\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.5216 Accuracy: 0.648438\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.4615 Accuracy: 0.632812\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.5873 Accuracy: 0.687500\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.4622 Accuracy: 0.609375\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.4036 Accuracy: 0.687500\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.4831 Accuracy: 0.625000\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.4422 Accuracy: 0.648438\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.6063 Accuracy: 0.664062\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.4482 Accuracy: 0.609375\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.4161 Accuracy: 0.640625\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.5041 Accuracy: 0.648438\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.4238 Accuracy: 0.640625\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.5679 Accuracy: 0.617188\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.4308 Accuracy: 0.609375\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.4279 Accuracy: 0.664062\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.5122 Accuracy: 0.617188\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.4373 Accuracy: 0.585938\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.5702 Accuracy: 0.640625\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.4394 Accuracy: 0.585938\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.4096 Accuracy: 0.671875\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.5234 Accuracy: 0.601562\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.4526 Accuracy: 0.625000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.5782 Accuracy: 0.664062\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.4454 Accuracy: 0.609375\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.4044 Accuracy: 0.640625\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.4852 Accuracy: 0.671875\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.4732 Accuracy: 0.656250\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.5871 Accuracy: 0.617188\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.4329 Accuracy: 0.593750\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.3943 Accuracy: 0.648438\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.5051 Accuracy: 0.609375\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.4588 Accuracy: 0.640625\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.5572 Accuracy: 0.625000\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.4102 Accuracy: 0.562500\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.3641 Accuracy: 0.648438\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.4672 Accuracy: 0.656250\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.4367 Accuracy: 0.656250\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.5501 Accuracy: 0.648438\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.4300 Accuracy: 0.648438\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.3703 Accuracy: 0.640625\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.4861 Accuracy: 0.640625\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.4523 Accuracy: 0.648438\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.5556 Accuracy: 0.640625\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.4208 Accuracy: 0.609375\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.3818 Accuracy: 0.632812\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.4722 Accuracy: 0.656250\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.4532 Accuracy: 0.632812\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.5597 Accuracy: 0.617188\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.4227 Accuracy: 0.601562\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.3749 Accuracy: 0.687500\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.4862 Accuracy: 0.609375\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.4621 Accuracy: 0.632812\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.5291 Accuracy: 0.656250\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.4322 Accuracy: 0.593750\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.3753 Accuracy: 0.648438\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.4966 Accuracy: 0.632812\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.4193 Accuracy: 0.593750\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.5718 Accuracy: 0.617188\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.4105 Accuracy: 0.617188\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.3783 Accuracy: 0.648438\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.4603 Accuracy: 0.625000\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.4702 Accuracy: 0.625000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.5677 Accuracy: 0.632812\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.4646 Accuracy: 0.601562\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.3612 Accuracy: 0.632812\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.4588 Accuracy: 0.664062\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.4479 Accuracy: 0.601562\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     0.5470 Accuracy: 0.656250\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:     0.4347 Accuracy: 0.578125\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:     0.4071 Accuracy: 0.656250\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:     0.4769 Accuracy: 0.648438\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
